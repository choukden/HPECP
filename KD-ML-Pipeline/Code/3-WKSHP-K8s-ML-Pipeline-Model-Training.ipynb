{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline with KubeDirector - Lab 3\n",
    "## Build, train and test your model in a remote tenant-shared training cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lab Workflow:**\n",
    "\n",
    "In this lab: \n",
    "\n",
    "1. You will first create your tenant user's working directory on the shared persistent container storage needed in the ML pipeline to store the dataset, trained models and scoring scripts.\n",
    "\n",
    "2. You will then build, train and test the model on a dataset using a remote tenant-shared training cluster. \n",
    "\n",
    "#### About the Dataset\n",
    "The dataset is based on the 2019 New York City yellow cab trip data (approximately 375,000 trip records from January-June 2019). The dataset has many different properties (aka _\"X - features\"_) like the pickup time and location, the dropoff time and location, the trip distance, the number of passengers, and several other variables. The goal is to predict the taxi ride duration in NY City (the target aka _\"Y - label\"_) based on these features. For this workshop, you will be using a premade dataset that requires little preprocessing. \n",
    "\n",
    "#### About the Machine Learning Model algorithm\n",
    "Gradient boosting supervised machine learning algorithm with Python is used here to build the model that is capable of predicting the duration of taxi trips in New York city. Python libraries such as Numpy, Pandas, Scikit-learn, XGBoost are used to build the model. \n",
    "\n",
    "The machine learning workflow depicted in the diagram below follows the typical supervised machine learning workflow:\n",
    "- After loading the dataset, the ML algorithm separates data into features (the taxi ride properties) and label (the taxi ride duration). \n",
    "- Then the dataset is divided into two parts, one for training the model and one for testing the model. The typical split is 80/20.\n",
    "- The ML algorithm is then defined and the model is built with the training data set to learn from. \n",
    "- Once the model is trained, the ML algorithm tests the model by making predictions on the test data set with the test features. \n",
    "- Next, the model accuracy is evaluated by comparing the test predictions to the test labels. Error metrics such as RMSE are used to evaluate the predictions accuracy.\n",
    "- The trained model is finally saved to a file in the tenant user's working directory in the shared persistent container storage repository. In the lab part 4 of the workshop, the trained model will be loaded to the production deployment engine from the saved file to serve predictions on new data (Lab part 5 of the workshop).\n",
    "\n",
    "![ML-Workflow](ML-Workflow.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup your working directory:**\n",
    "\n",
    "The Python code here is used to create the working directory for your tenant userID. The working directory is used to store the key data components needed in the ML pipeline such as the input dataset, trained ML model(s) and scoring script(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository /bd-fs-mnt/TenantShare/repo/models/NYCTaxi/student74 exists for user student74. Deleting the repo now...\n",
      "Repository /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/student74 exists for user student74. Deleting the repo now...\n"
     ]
    }
   ],
   "source": [
    "#userID = \"student{{ STDID }}\"\n",
    "\n",
    "userID = \"student74\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Shared dataset across all tenant users, use case directory name and scoring file name\n",
    "datasetFile = \"demodata.csv\"\n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "scoringFile=\"XGB_Scoring.py\"\n",
    "scoringFileV2=\"XGB_Scoringv2.py\"\n",
    "\n",
    "# Project repo path function - file system mount available to all app containers\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "# Delete existing working directory for userID (that may exists from previous execution of the workshop)\n",
    "studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "studentRepoCode = \"code\" + '/' + usecaseDirectory + '/' + userID\n",
    "pathModel = ProjectRepo(studentRepoModel)\n",
    "pathCode = ProjectRepo(studentRepoCode)\n",
    "\n",
    "if (os.path.exists(ProjectRepo(studentRepoModel))):\n",
    "    print (\"Repository \" + pathModel + \" exists for user \" + userID + \". Deleting the repo now...\")\n",
    "    shutil.rmtree(pathModel, ignore_errors=True)\n",
    "        \n",
    "if (os.path.exists(ProjectRepo(studentRepoCode))):\n",
    "    print (\"Repository \" + pathCode + \" exists for user \" + userID + \". Deleting the repo now...\")\n",
    "    shutil.rmtree(pathCode, ignore_errors=True)\n",
    "    \n",
    "# Making sure the input Dataset is loaded and accessible in the shared persistent container storage for your tenant\n",
    "# Check the dataset file exists in /db-fs-mnt/TenantShare/repo/data/NYCTaxi folder:\n",
    "#print (os.listdir(ProjectRepo('data/NYCTaxi')))\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "pathData = ProjectRepo(\"data\" + '/' + usecaseDirectory)\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(datasetFilePath))):\n",
    "    print (\"Error! Dataset file \" + ProjectRepo(datasetFilePath) + \" does not exist. Please contact the workshop administrator at hpedev.hackshack@hpe.com before continuing.\")\n",
    "if (not os.path.exists(ProjectRepo(locationFilePath))):\n",
    "    print (\"Error! location table file \" + ProjectRepo(locationFilePath) + \" does not exist. Please contact the workshop administrator at hpedev.hackshack@hpe.com before continuing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating your working directory structure to store the trained models and scoring code scripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model repository /bd-fs-mnt/TenantShare/repo/models/NYCTaxi/student74 does not exist for user. Creating the directory now...\n",
      "Successfully created the model directory /bd-fs-mnt/TenantShare/repo/models/NYCTaxi/student74\n",
      "Code repository for user /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/student74 does not exist for user. Creating the directory now...\n",
      "Successfully created the code directory /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/student74\n",
      "File/bd-fs-mnt/TenantShare/repo/code/NYCTaxi/student74/XGB_Scoring.py does not exist.\n",
      "Copying it now on /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/student74 and setting file permissions\n",
      "File/bd-fs-mnt/TenantShare/repo/code/NYCTaxi/student74/XGB_Scoringv2.py does not exist.\n",
      "Copying it now on /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/student74 and setting file permissions\n",
      "target directory  /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/student74 :\n",
      "['XGB_Scoringv2.py', 'XGB_Scoring.py']\n"
     ]
    }
   ],
   "source": [
    "# Define the directory structure for the UserID to store trained model and the scoring script\n",
    "# /bd-fs-mnt/TenantShare/repo/models/NYCTaxi/userID; /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/userID\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(studentRepoModel))):\n",
    "    print (\"Model repository \" + pathModel + \" does not exist for user. Creating the directory now...\")\n",
    "    try:\n",
    "        os.makedirs(pathModel)\n",
    "    except OSError:\n",
    "        print (\"Creation of the model directory %s failed\" % pathModel)\n",
    "    else:\n",
    "        print (\"Successfully created the model directory %s\" % pathModel)\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(studentRepoCode))):\n",
    "    print (\"Code repository for user \" + pathCode + \" does not exist for user. Creating the directory now...\")\n",
    "    try:\n",
    "        os.makedirs(pathCode)\n",
    "    except OSError:\n",
    "        print (\"Creation of the code directory %s failed\" % pathCode)\n",
    "    else:\n",
    "        print (\"Successfully created the code directory %s\" % pathCode)\n",
    "        \n",
    "# Copying scoring script files to your code repository in your working directory\n",
    "# Make sure the scoring script files are available in the /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/userID folder with appropriate permissions (read-execute)\n",
    "##print (\"local directory on your local Jupyter Notebook:\")\n",
    "##print (os.listdir(os.curdir))\n",
    "##print (\"target directory on the shared File System mount for your tenant: \")\n",
    "##print (os.listdir(pathCode))\n",
    "srcFile = scoringFile\n",
    "destFile = pathCode + '/' + scoringFile\n",
    "srcFileV2 = scoringFileV2\n",
    "destFileV2 = pathCode + '/' + scoringFileV2\n",
    "\n",
    "if (not os.path.exists(scoringFile)):\n",
    "    print (scoringFile + \" does not exist in your local Jupyter Notebook! Please make sure to copy the file \" + scoringFile + \" to your local Jupyter Notebook, then re-run that code cell to continue the lab.\")\n",
    "else:    \n",
    "    if (os.path.exists(pathCode + '/' + scoringFile)):\n",
    "        print (pathCode + '/' + scoringFile + \" file exists. Setting permissions\")\n",
    "        os.chmod (destFile, 0o777)\n",
    "    else:\n",
    "        print (\"File\" + pathCode + '/' + scoringFile + \" does not exist.\")\n",
    "        print (\"Copying it now on \" + pathCode + \" and setting file permissions\")\n",
    "        try:\n",
    "            shutil.copy(srcFile, destFile)\n",
    "            os.chmod (destFile, 0o777)\n",
    "        except IOError as e:\n",
    "            print (\"Unable to copy scoring file. %s\" % e )\n",
    "        except:\n",
    "            print (\"Unexpected error:\", sys.exec_info())\n",
    "\n",
    "if (not os.path.exists(scoringFileV2)):\n",
    "    print (scoringFileV2 + \" does not exist in your local Jupyter Notebook! Please make sure to copy the file \" + scoringFileV2 + \" to your local Jupyter Notebook, then re-run that code cell to continue the lab.\")\n",
    "else:    \n",
    "    if (os.path.exists(pathCode + '/' + scoringFileV2)):\n",
    "        print (pathCode + '/' + scoringFileV2 + \" file exists. Setting permissions\")\n",
    "        os.chmod (destFileV2, 0o777)\n",
    "    else:\n",
    "        print (\"File\" + pathCode + '/' + scoringFileV2 + \" does not exist.\")\n",
    "        print (\"Copying it now on \" + pathCode + \" and setting file permissions\")\n",
    "        try:\n",
    "            shutil.copy(srcFileV2, destFileV2)\n",
    "            os.chmod (destFileV2, 0o777)\n",
    "        except IOError as e:\n",
    "            print (\"Unable to copy scoring file. %s\" % e )\n",
    "        except:\n",
    "            print (\"Unexpected error:\", sys.exec_info())\n",
    "            \n",
    "print (\"target directory  \" + pathCode + \" :\")\n",
    "print (os.listdir(pathCode))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Connection to the tenant-shared Training Cluster:**\n",
    "\n",
    "Next, you'll test that the communication with the shared training cluster is indeed functioning properly.\n",
    "\n",
    "> **Note:** _The KubeDirectorApp Jupyter Notebook used in our solution includes **custom magic** functions to handle remotely submitting training code and retrieving results and logs. The Notebook uses these magic functions to make REST API calls to the API server that runs as part of the shared training environment. These calls submit training jobs and get results from within the Notebook session._\n",
    "\n",
    "The Jupyter Notebook kdapp includes the following **line magic** functions: \n",
    "*\t%attachments: Returns a list of connected training environments. \n",
    "*\t%logs --url: URL of the training server load balancer used to monitor the status of the training job.\n",
    "\n",
    "The Jupyter Notebook kdapp also includes the following **cell magic** function:\n",
    "*\t%%training_cluster_name \n",
    "This would submit Training code to the shared Training environment\n",
    "\n",
    "<b>%attachments</b> is a line magic command that output a table with the name(s) of the training cluster(s) available for us to use. Sometimes, tenant admin may have created multiple training clusters for different projects depending on the needs of the model or size of data, e.g. some with GPU nodes, while others with CPUs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cluster      ML Engine\n",
      "--------------------  -----------\n",
      "trainingengineshared  python\n"
     ]
    }
   ],
   "source": [
    "%attachments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize the training cluster, you will need grab the name of the training cluster you want to use and feed it into another custom line magic command. \n",
    "\n",
    "In this lab, you are going to use the **trainingengineshared** training cluster that is the shared training environment for all training jobs in the workshop. \n",
    "\n",
    "The Jupyter notebook will then send the contents of the cell to be executed on the training cluster. \n",
    "The example cell below will execute a print statement on the training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History URL: http://kdss-8jsj7-0.kdhs-djs2v.k8smltenant.svc.cluster.local:10001/history/6\n"
     ]
    }
   ],
   "source": [
    "%%trainingengineshared\n",
    "\n",
    "import datetime\n",
    "\n",
    "print('test')\n",
    "print(\"Date time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training cluster will send back a unique log url to this particular user and notebook. \n",
    "You can use this URL with another custom **line magic** command to track the status of the job in real time. \n",
    "\n",
    "Copy the HTTP URL output from the output of the previous cell and paste it into the cell below where it says _\"your_http_url_here\"_, and run the cell code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: Finished\n",
      "test\n",
      "Date time:  2020-12-16 19:10:37.100726\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##%logs --url your_http_url_here\n",
    "%logs --url http://kdss-8jsj7-0.kdhs-djs2v.k8smltenant.svc.cluster.local:10001/history/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model remotely on a shared training cluster:\n",
    "Run the cell code below to build, train, test your model. At the end of this code cell, the model is saved to a file in your tenant user's working directory for later use in production deployment engine.  \n",
    "\n",
    "Make sure you fill your <b>training cluster name</b> in the cell magic (_%%trainingengineshared_)! to execute the code cell below on the remote shared training cluster. If you comment this magic command, the code will run on your local Jupyter notebook and the training process may take longer time to complete because the local jupyter notebook is a small capacity server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History URL: http://kdss-bwffl-0.kdhs-djs2v.k8smltenant.svc.cluster.local:10001/history/7\n"
     ]
    }
   ],
   "source": [
    "%%trainingengineshared\n",
    "\n",
    "#userID = \"student{{ STDID }}\"\n",
    "\n",
    "userID = \"student74\"\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "datasetFile = \"demodata.csv\"\n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "\n",
    "# Start time \n",
    "print(\"Start time for \" + userID + \": \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "print(\"Reading in data\")\n",
    "# Reading in dataset table using pandas\n",
    "dbName = \"pqyellowtaxi\"\n",
    "##df = pd.read_csv(ProjectRepo('data/NYCTaxi/demodata.csv'))\n",
    "df = pd.read_csv(ProjectRepo(datasetFilePath))\n",
    "\n",
    "# Reading in latitude/longitude coordinate lookup table using pandas \n",
    "lookupDbName = \"pqlookup\"\n",
    "##dflook = pd.read_csv(ProjectRepo('data/NYCTaxi/lookup-ipyheader.csv'))\n",
    "dflook = pd.read_csv(ProjectRepo(locationFilePath))\n",
    "print(\"Done reading in data\")\n",
    "\n",
    "\n",
    "# merging dataset and lookup tables on latitudes/coordinates\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "\n",
    "\n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# convert string to datetime\n",
    "df[fullName('tpep_pickup_datetime')] = pd.to_datetime(df[fullName('tpep_pickup_datetime')])\n",
    "df[fullName('tpep_dropoff_datetime')] = pd.to_datetime(df[fullName('tpep_dropoff_datetime')])\n",
    "df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "# feature engineering\n",
    "# Feature engineering is the process of transforming raw data into inputs for a machine learning algorithm\n",
    "df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "df[fullName(\"month\")] = df[fullName('tpep_pickup_datetime')].dt.month\n",
    "# convert month to a categorical feature using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=[fullName(\"month\")])\n",
    "\n",
    "# Filter dataset to rides under 3 hours and under 150 miles to remove outliers\n",
    "df = df[df[fullName('duration')] > 20]\n",
    "df = df[df[fullName('duration')] < 10800]\n",
    "df = df[df[fullName('trip_distance')] > 0]\n",
    "df = df[df[fullName('trip_distance')] < 150]\n",
    "\n",
    "# drop null rows\n",
    "df = df.dropna(how='any',axis=0)\n",
    "\n",
    "# select columns to be used as features\n",
    "cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour')]\n",
    "cols.extend([fullName('month_' + str(x)) for x in range(1, 7)])\n",
    "cols.append(fullName('duration'))\n",
    "dataset = df[cols]\n",
    "\n",
    "# separate data into features (the taxi ride properties) and label (duration) using .iloc\n",
    "X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "del dataset\n",
    "del df\n",
    "\n",
    "print(\"Done cleaning data\")\n",
    "\n",
    "\n",
    "print(\"Training and testing...\")\n",
    "\n",
    "# As we have one dataset, the data is split into a training data set and a test data set. The ideal split is 80:20. \n",
    "# 80% of the data is used to train the model and 20% is used for testing the model.\n",
    "print(\"Load the sklearn module to split the dataset into a training data set and a test data set.\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Build the model and fit the model on the training data. This will take a few minutes...\")\n",
    "\n",
    "#Define the ML algorithm (that is the learning algorithm to use). We use here the XGBRegressor class of the xgboost package to build the model\n",
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror', \n",
    "                        colsample_bytree = 1,\n",
    "                        subsample = 1,\n",
    "                        learning_rate = 0.15,\n",
    "                        booster = \"gbtree\",\n",
    "                        max_depth = 3,\n",
    "                        eta = 0.5,\n",
    "                        eval_metric = \"rmse\",) \n",
    "\n",
    "print(\"num train elements: \" + str(len(X_train)))\n",
    "\n",
    "# then train (fit) the model on training data set comprised of inputs (features) and outputs (label)\n",
    "# we provide our defined ML algorithm with data to learn from\n",
    "print(\"Train start time: \", datetime.datetime.now())\n",
    "xgbr.fit(X_train, y_train)\n",
    "#after training the model, check the model training score. The closer towards 1, the better the fit.\n",
    "score = xgbr.score(X_train, y_train)  \n",
    "print(\"Model training score. The closer towards 1, the better the fit: \", score)\n",
    "\n",
    "print(\"Train end time: \", datetime.datetime.now())\n",
    "print(\"Test the model by making predictions on the test data set where only the features are provided\")\n",
    "y_pred = xgbr.predict(X_test)\n",
    "y_pred = y_pred.clip(min=0)\n",
    "\n",
    "print(\"Evaluating the model accuracy by comparing the test predictions with the test labels. RMSE is used as evaluation metric.\")\n",
    "# evaluating the model accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Root Mean Squared Log Error:', np.sqrt(mean_squared_log_error( y_test, y_pred)))\n",
    "print()\n",
    "print(\"Note that for RMSE the lower that value is, the better the fit\")\n",
    "\n",
    "#after we have trained the model, save it in a pickle file in the tenant user's working directory for later use in production deployment engine\n",
    "#the model will be loaded back from the pickle file using the python scoring script in the deployment engine to make predictions on new data.\n",
    "print(\"Saving model in a pickle file as \" + ProjectRepo(studentRepoModel) + '/' + \"XGB.pickle.dat\")\n",
    "pickle.dump(xgbr, open( ProjectRepo(studentRepoModel) + '/' + \"XGB.pickle.dat\", \"wb\"))\n",
    "\n",
    "# Finish time\n",
    "print(\"End time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Monitor the training job:**\n",
    "\n",
    "#### Copy the unique log url above and paste it into the cell below to monitor your training job (repeat regularly)\n",
    "\n",
    "The model training will take a few minutes to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: Finished\n",
      "Importing libraries\n",
      "Start time for student74:  2020-12-16 19:10:58.878378\n",
      "Reading in data\n",
      "Done reading in data\n",
      "(330407, 14)\n",
      "(330407,)\n",
      "Done cleaning data\n",
      "Training and testing...\n",
      "Load the sklearn module to split the dataset into a training data set and a test data set.\n",
      "Build the model and fit the model on the training data. This will take a few minutes...\n",
      "num train elements: 264325\n",
      "Train start time:  2020-12-16 19:11:10.809347\n",
      "Model training score. The closer towards 1, the better the fit:  0.8095123420527638\n",
      "Train end time:  2020-12-16 19:11:55.025369\n",
      "Test the model by making predictions on the test data set where only the features are provided\n",
      "Evaluating the model accuracy by comparing the test predictions with the test labels. RMSE is used as evaluation metric.\n",
      "Mean Absolute Error: 195.8130179028589\n",
      "Mean Squared Error: 92811.54031166565\n",
      "Root Mean Squared Error: 304.64986511020425\n",
      "Root Mean Squared Log Error: 0.3464091230815159\n",
      "\n",
      "Note that for RMSE the lower that value is, the better the fit\n",
      "Saving model in a pickle file as /bd-fs-mnt/TenantShare/repo/models/NYCTaxi/student74/XGB.pickle.dat\n",
      "End time:  2020-12-16 19:11:55.513187\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##%logs --url your_http_url_here\n",
    "%logs --url  http://kdss-bwffl-0.kdhs-djs2v.k8smltenant.svc.cluster.local:10001/history/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model registry and deployment:**\n",
    "\n",
    "##### After the model development and training, you are now ready to deploy your prediction service using the trained model, and start serving queries.\n",
    "\n",
    "Now, let's go back to your JupyterHub account session to continue the hands-on from lab 4  \n",
    "**(4-WKSHP-K8s-ML-Pipeline-Register-Model-Deployment.ipynb)** for model registry and model deployment.\n",
    "\n",
    "You will use again your local Jupyter Notebook later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain the model to improve model accuracy\n",
    "Run the cell code below to retrain your model. The \"max_depth\" XGBoost parameter is set to a higher value (value=6 instead of 3) to improve the model accuracy. The model will be saved to a file as XGB.picklev2.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History URL: http://kdss-bwffl-0.kdhs-djs2v.k8smltenant.svc.cluster.local:10001/history/9\n"
     ]
    }
   ],
   "source": [
    "%%trainingengineshared\n",
    "\n",
    "#userID = \"student{{ STDID }}\"\n",
    "\n",
    "userID = \"student74\"\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "datasetFile = \"demodata.csv\"\n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "\n",
    "# Start time \n",
    "print(\"Start time for \" + userID + \": \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "print(\"Reading in data\")\n",
    "# Reading in dataset table using pandas\n",
    "dbName = \"pqyellowtaxi\"\n",
    "##df = pd.read_csv(ProjectRepo('data/NYCTaxi/demodata.csv'))\n",
    "df = pd.read_csv(ProjectRepo(datasetFilePath))\n",
    "\n",
    "# Reading in latitude/longitude coordinate lookup table using pandas \n",
    "lookupDbName = \"pqlookup\"\n",
    "##dflook = pd.read_csv(ProjectRepo('data/NYCTaxi/lookup-ipyheader.csv'))\n",
    "dflook = pd.read_csv(ProjectRepo(locationFilePath))\n",
    "print(\"Done reading in data\")\n",
    "\n",
    "\n",
    "# merging dataset and lookup tables on latitudes/coordinates\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "\n",
    "\n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# convert string to datetime\n",
    "df[fullName('tpep_pickup_datetime')] = pd.to_datetime(df[fullName('tpep_pickup_datetime')])\n",
    "df[fullName('tpep_dropoff_datetime')] = pd.to_datetime(df[fullName('tpep_dropoff_datetime')])\n",
    "df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "# feature engineering\n",
    "# Feature engineering is the process of transforming raw data into inputs for a machine learning algorithm\n",
    "df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "df[fullName(\"month\")] = df[fullName('tpep_pickup_datetime')].dt.month\n",
    "# convert month to a categorical feature using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=[fullName(\"month\")])\n",
    "\n",
    "# Filter dataset to rides under 3 hours and under 150 miles to remove outliers\n",
    "df = df[df[fullName('duration')] > 20]\n",
    "df = df[df[fullName('duration')] < 10800]\n",
    "df = df[df[fullName('trip_distance')] > 0]\n",
    "df = df[df[fullName('trip_distance')] < 150]\n",
    "\n",
    "# drop null rows\n",
    "df = df.dropna(how='any',axis=0)\n",
    "\n",
    "# select columns to be used as features\n",
    "cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour')]\n",
    "cols.extend([fullName('month_' + str(x)) for x in range(1, 7)])\n",
    "cols.append(fullName('duration'))\n",
    "dataset = df[cols]\n",
    "\n",
    "# separate data into features (the taxi ride properties) and label (duration) using .iloc\n",
    "X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "del dataset\n",
    "del df\n",
    "\n",
    "print(\"Done cleaning data\")\n",
    "\n",
    "\n",
    "print(\"Training and testing...\")\n",
    "\n",
    "# As we have one dataset, the data is split into a training data set and a test data set. The ideal split is 80:20. \n",
    "# 80% of the data is used to train the model and 20% is used for testing the model.\n",
    "print(\"Split dataset into a training data set and a test data set.\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Build the model and fit the model on the training data. This will take a few minutes...\")\n",
    "\n",
    "#use the XGBRegressor class of the xgboost package to build the model\n",
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror', \n",
    "                        colsample_bytree = 1,\n",
    "                        subsample = 1,\n",
    "                        learning_rate = 0.15,\n",
    "                        booster = \"gbtree\",\n",
    "                        max_depth = 6,\n",
    "                        eta = 0.5,\n",
    "                        eval_metric = \"rmse\",) \n",
    "\n",
    "print(\"num train elements: \" + str(len(X_train)))\n",
    "\n",
    "# then train (fit) the model on training data set comprised of inputs (features) and outputs (label)\n",
    "print(\"Train start time: \", datetime.datetime.now())\n",
    "xgbr.fit(X_train, y_train)\n",
    "#after training the model, check the model training score. The closer towards 1, the better the fit.\n",
    "score = xgbr.score(X_train, y_train)  \n",
    "print(\"Model training score. The closer towards 1, the better the fit: \", score)\n",
    "\n",
    "print(\"Train end time: \", datetime.datetime.now())\n",
    "print(\"Test the model by making predictions on the test data set where only the features are provided\")\n",
    "y_pred = xgbr.predict(X_test)\n",
    "y_pred = y_pred.clip(min=0)\n",
    "\n",
    "print(\"Evaluating the model accuracy by comparing the test predictions with the test labels\")\n",
    "# evaluating the model accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Root Mean Squared Log Error:', np.sqrt(mean_squared_log_error( y_test, y_pred)))\n",
    "print()\n",
    "print(\"Note that for RMSE the lower that value is, the better the fit\")\n",
    "\n",
    "\n",
    "print(\"Saving model as \" + ProjectRepo(studentRepoModel) + '/' + \"XGB.picklev2.dat\")\n",
    "pickle.dump(xgbr, open( ProjectRepo(studentRepoModel) + '/' + \"XGB.picklev2.dat\", \"wb\"))\n",
    "\n",
    "\n",
    "# Finish time\n",
    "print(\"End time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the training job \n",
    "\n",
    "#### Copy the unique log url above and paste it into the cell below to monitor your training job (repeat regularly)\n",
    "\n",
    "The model training will take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: Finished\n",
      "Importing libraries\n",
      "Start time for student74:  2020-12-16 19:28:53.625921\n",
      "Reading in data\n",
      "Done reading in data\n",
      "(330407, 14)\n",
      "(330407,)\n",
      "Done cleaning data\n",
      "Training and testing...\n",
      "Split dataset into a training data set and a test data set.\n",
      "Build the model and fit the model on the training data. This will take a few minutes...\n",
      "num train elements: 264325\n",
      "Train start time:  2020-12-16 19:29:05.644290\n",
      "Model training score. The closer towards 1, the better the fit:  0.852684422668263\n",
      "Train end time:  2020-12-16 19:30:44.888571\n",
      "Test the model by making predictions on the test data set where only the features are provided\n",
      "Evaluating the model accuracy by comparing the test predictions with the test labels\n",
      "Mean Absolute Error: 176.4289201526949\n",
      "Mean Squared Error: 76454.33810087029\n",
      "Root Mean Squared Error: 276.50377592515855\n",
      "Root Mean Squared Log Error: 0.3103454404177983\n",
      "\n",
      "Note that for RMSE the lower that value is, the better the fit\n",
      "Saving model as /bd-fs-mnt/TenantShare/repo/models/NYCTaxi/student74/XGB.picklev2.dat\n",
      "End time:  2020-12-16 19:30:45.577701\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##%logs --url your_http_url_here\n",
    "%logs --url http://kdss-bwffl-0.kdhs-djs2v.k8smltenant.svc.cluster.local:10001/history/9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, you can logout from your local jupyter notebook server.  \n",
    "\n",
    "### Then go back to your JupyterHub account session to continue the hands-on from lab 5 and adjust the model registry information, section _\"Dynamic? Did someone say Dynamic ML pipeline?\"_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
